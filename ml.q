\d .ml

addint:{((1;count x 0)#1f),x}   / add intercept

predict:{[X;THETA]THETA$addint X} / regression predict

/ regularized linear regression cost
rlincost:{[l;X;Y;THETA]
 J:sum (1f%2*n:count Y 0)*sum Y$/:Y-:predict[X;THETA];
 if[l>0f;J+:(l%2*n)*x$x:raze @[;0;:;0f]'[THETA]];
 J}
lincost:rlincost[0f]

/ regularized linear regression gradient
rlingrad:{[l;X;Y;THETA]
 g:(1f%n:count Y 0)*addint[X]$/:predict[X;THETA]-Y;
 if[l>0f;g+:(l%n)*@[;0;:;0f]'[THETA]];
 g}
lingrad:rlingrad[0f]

/ gf: gradient function
gd:{[alpha;gf;THETA] THETA-alpha*gf THETA} / gradient descent

mlsq:{flip inv[y$/:y]$x$/:y}    / normal equations

zscore:{(x-avg x)%dev x}        / feature normalization

sigmoid:{1f%1f+exp neg x}       / sigmoid function

lpredict:(')[sigmoid;predict]   / logistic regression predict

/ logistic regression cost
lcost:{sum (-1f%count y 0)*sum each (y*log x)+(1f-y)*log 1f-x}

/ regularized logistic regression cost
rlogcost:{[l;X;Y;THETA]
 J:lcost[X lpredict/ THETA;Y];
 if[l>0f;J+:(l%2*count Y 0)*x$x:2 raze/ @[;0;:;0f]''[THETA]]; / regularization
 J}
logcost:rlogcost[0f]

/ regularized logistic regression gradient
rloggrad:{[l;X;Y;THETA]
 n:count Y 0;
 a:lpredict\[enlist[X],THETA];
 D:last[a]-Y;
 a:addint each -1_a;
 D:{[D;THETA;a]1_(flip[THETA]$D)*a*1f-a}\[D;reverse 1_THETA;reverse 1_a],enlist D;
 g:(a($/:)'D)%n;
 if[l>0f;g+:(l%n)*@[;0;:;0f]''[THETA]]; / regularization
 g}
loggrad:rloggrad[0f]

rlogcostgrad:{[l;X;Y;THETA]
 J:sum rlogcost[l;X;Y;2 enlist/ THETA];
 g:2 raze/ rloggrad[l;X;Y;2 enlist/ THETA];
 (J;g)}
logcostgrad:rlogcostgrad[0f]

rlogcostgradf:{[l;X;Y]
 Jf:(sum rlogcost[l;X;Y]enlist enlist@);
 gf:(raze rloggrad[l;X;Y]enlist enlist @);
 (Jf;gf)}
logcostgradf:rlogcostgradf[0f]

/ normalized initialization - Glorot and Bengio (2010)
ninit:{sqrt[6f%x+y]*-1f+(x+:1)?/:y#2f}

/ (m)inimization (f)unction, (c)ost (g)radient (f)unction
onevsall:{[mf;cgf;Y;lbls] (mf cgf "f"$Y=) peach lbls}

imax:{x?max x}                  / index of max element
imin:{x?min x}                  / index of min element

/ predict each number and pick best
predictonevsall:{[X;THETA]imax each flip X lpredict/ THETA}

/ confusion matrix
cm:{
 n:count u:asc distinct x,y;
 m:.[;;1+]/[(n;n)#0;flip (u?y;u?x)];
 t:([]x:u)!flip (`$string[u])!m;
 t}

/ cut a vector into n matrices
mcut:{[n;x](1+-1_n) cut' (sums {x*y+1} prior -1_n) cut x}
diag:{$[0h>t:type x;x;@[n#abs[t]$0;;:;]'[til n:count x;x]]}

/ (f)unction, x, (e)psilon
/ compute partial derivatives if e is a list
numgrad:{[f;x;e](.5%e)*{x[y+z]-x[y-z]}[f;x] peach diag e}

checknngradients:{[l;n]
 theta:2 raze/ THETA:ninit'[-1_n;1_n];
 X:flip ninit[-1+n 0;n 1];
 y:1+(1+til n 1) mod last n;
 YMAT:flip diag[last[n]#1f]"i"$y-1;
 g:2 raze/ rloggrad[l;X;YMAT] THETA; / analytic gradient
 f:(rlogcost[l;X;YMAT]mcut[n]@);
 ng:numgrad[f;theta] count[theta]#1e-4; / numerical gradient
 (g;ng)}

/ n can be any network topology dimension
nncost:{[l;n;X;YMAT;theta] / combined cost and gradient for efficiency
 THETA:mcut[n] theta;
 Y:last a:lpredict\[enlist[X],THETA];
 n:count YMAT 0;
 J:lcost[Y;YMAT];
 if[l>0f;J+:(l%2*n)*{x$x}2 raze/ @[;0;:;0f]''[THETA]]; / regularization
 D:Y-YMAT;
 a:addint each -1_a;
 D:{[D;THETA;a]1_(flip[THETA]$D)*a*1f-a}\[D;reverse 1_THETA;reverse 1_a],enlist D;
 g:(a($/:)'D)%n;
 if[l>0f;g+:(l%n)*@[;0;:;0f]''[THETA]]; / regularization
 (J;2 raze/ g)}

nncostf:{[l;n;X;YMAT]
 Jf:(first nncost[l;n;X;YMAT]@);
 gf:(last nncost[l;n;X;YMAT]@);
 (Jf;gf)}

/ stochastic gradient descent

/ successively call (m)inimization (f)unction with (THETA) and
/ randomly sorted (n)-sized chunks generated by (s)ampling (f)unction
sgd:{[mf;sf;n;X;THETA]THETA mf/ n cut sf count X 0}

/ k-means

edist:{sum x*x-:y}              / euclidian distance
mdist:{sum abs x-y}             / manhattan distance (taxicab metric)
hmean:{1f%avg 1f%x}             / harmonic mean

cossim:{(sum x*y)%sqrt(sum x*x)*sum y*y} / cosine similarity
cosdist:(')[1f-;cossim]                  / cosine distance

/ using the (d)istance (f)unction, cluster the data (X) into groups
/ defined by the closest (C)entroid
cgroup:{[df;X;C] group imin each flip df[X] each flip C}

/ k-(means|medians) algorithm

/ stuart lloyd's algorithm. using a (d)istance (f)unction and
/ (m)ean/edian (f)unction, find (k)-centroids in the data (X) starting
/ with a (C)entroid list. if C is an atom, use it to randomly
/ initialize C. if negative, use "Forgy" method and randomly pick k
/ centroids.  if positive, use "Random Partition" method to randomly
/ assign to k clusters.
lloyd:{[df;mf;X;C]
 if[0h>type C;if[0>C;C:X@\:C?count X 0]];
 g:$[0h>type C;group count[X 0]?C;cgroup[df;X;C]]; / assignment step
 C:mf''[X@\:value g];                              / update step
 C}

kmeans:lloyd[edist;avg]
kmedians:lloyd[mdist;med]
khmeans:lloyd[edist;hmean]

/ using the (d)istance (f)unction, cluster the data (X) into groups
/ defined by the closest (C)entroid and return the distance
cdist:{[df;X;C] k!df[X@\:value g] C@\:k:key g:cgroup[df;X;C]}
ecdist:cdist[edist]
mcdist:cdist[mdist]

distortion:sum sum each

/ ungroup (inverse of group)
ugrp:{(key[x] where count each value x)iasc raze x}

/ lance-williams algorithm update functions
single:{.5 .5 0 -.5}
complete:{.5 .5 0 .5}
average:{(x%sum x _:2),0 0f}
weighted:{.5 .5 0 0}
centroid:{((x,neg prd[x]%s)%s:sum x _:2),0f}
ward:{((k+/:x 0 1),(neg k:x 2;0f))%\:sum x}

/ implementation of lance-williams algorithm for performing
/ hierarchical agglomerative clustering. given (l)inkage (f)unction to
/ determine distance between new and remaining clusters and
/ (d)issimilarity (m)atrix, return (from;to;distance;#elements).  lf
/ in `single`complete`average`weighted`centroid`ward
lw:{[lf;dm]
 n:count dm 0;
 if[0w=d@:i:imin d:(n#dm)@'dm n;:dm]; / find closest clusters
 j:dm[n] i;                           / find j
 c:lf (count each group dm[n+1])@/:(i;j;til n); / determine coefficients
 nd:sum c*nd,d,enlist abs(-/)nd:dm(i;j);        / calc new distances
 dm[til n;i]:dm[i]:nd;                          / update distances
 dm[i;i]:0w;                                    / fix diagonal
 dm[j;(::)]:0w;                                 / erase j
 dm[til n+2;j]:(n#0w),i,i;    / erase j and set aux data
 dm[n]:imin each n#dm;        / find next closest element
 dm[n+1;where j=dm n+1]:i;    / all elements in cluster j are now in i
 dm:@[dm;n+2 3 4 5;,;(j;i;d;count where i=dm n+1)];
 dm}

/ given a (d)istance (f)unction and (l)inkage (f)unction, construct the
/ linkage (dendrogram) statistics of data in X
linkage:{[df;lf;X]
 dm:df[X] each flip X;                       / dissimilarity matrix
 dm:.[;;:;0w]/[dm;flip (i;i:til count X 0)]; / ignore loops
 dm,:enlist imin each dm;
 dm,:enlist til count dm 0;
 dm,:4#();
 l:-4#lw[lf] over dm;
 l}

/ merge node y[0] into y[1] in tree x
graft:{@[x;y;:;(::;x y)]}

/ build a complete dendrogram from linkage data x
tree:{1#(til[1+count x],(::)) graft/ x}

/ cut a single layer off tree
slice:{$[type x;x;type f:first x;(1_x),f;type ff:first f;(1_f),(1_x),ff;f,1_x]}

/ binomial pdf (not atomic because of factorial)
binpdf:{[n;k;p]
 if[0<max type each (n;p;k);:.z.s'[n;k;p]];
 r:prd[1+k+til n]%prd 1+til n-:k;
 r*:prd (p;1f-p) xexp (k;n);
 r}

/ binomial log likelihood
binll:{[n;k;p](k*log p)+(n-k)*log 1f-p}
/ binomial likelihood approximation (without the coefficient)
binla:{[n;k;p](p xexp k)*(1f-p) xexp n-k}
/ binomial maximum likelihood
binml:{[n;x;w]{(1#x)%sum x}w$/:"f"$(x;n-x)}

/ gaussian kernel
gaussk:{[mu;s;x] exp (sum x*x-:mu)%-2*s*s}

/ gaussian
gauss:{[mu;s;x]
 p:exp (x*x-:mu)%-2*s*s;
 p%:s*sqrt 2f*acos -1f;
 p}

/ gaussian multivariate
gaussmv:{[mu;s2;X]
 if[type s2;s2:diag count[X]#s2];
 p:exp -.5*sum X*inv[s2]$X-:mu;
 p*:sqrt 1f%.qml.mdet s2;
 p*:(2f*acos -1f) xexp -.5*count X;
 p}

/ gaussian maximum likelihood
gaussml:{[X;w](mu;sqrt w wavg X*X-:mu:w wavg X)}
/ gaussian maximum likelihood multi variate
gaussmlmv:{[X;w](mu;w wavg X (*\:/:)' X:flip X-mu:w wavg/: X)}

/ guassian log likelihood
gaussll:{[mu;s2;X] -.5*sum (log 2*acos -1f;log s2;(X*X-:mu)%s2)}

/ (l)ikelhood (f)unction, (m)aximization (f)unction
/ with prior probabilities (p)hi and distribution parameters (t)heta
em:{[lf;mf;X;pt]
 if[0>type pt;pt:enlist pt#1f%pt]; / default to equal prior probabilities
 l:$[1<count pt;(lf[X] .) peach flip 1_pt;count[$[type X;X;X 0]]?/:count[pt 0]#1f];
 W:p%\:sum p:l*phi:pt 0;         / weights (responsibilities)
 if[0h<type phi;phi:avg each W]; / new prior probabilities (if phi is a list)
 theta:flip mf[X] peach W;       / new coefficients
 enlist[phi],theta}

/ return value(s) which occur most frequently
mode:{where max[x]=x:count each group x}

/ k nearest neighbors

/ pick k closest values to x from training data X and return the
/ (c)lassification that occurs most frequently
knn:{[df;k;c;X;x]first mode c k#iasc df[X;x]}

/ markov clusetering
/ if type of X is not a real or float, add loops and normalize
/ (p)rune is an integer, take p largest, otherwise take everything > p
mcl:{[e;r;p;X]
 if[8h>type X 0;X%:sum each X|:diag count[X]#1b];
 X:xexp[(e-1)$[X]/X;r];
 X*:$[-8h<type p;(p>iasc idesc@)';p<]X;
 X%:sum each X;
 X}
